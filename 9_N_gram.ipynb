{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8bd4b0",
   "metadata": {},
   "source": [
    "### N gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5dccb",
   "metadata": {},
   "source": [
    "An N-gram is a sequence of N continuous items from a text (words or characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af918cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eg :: bigram , trigram\n",
    "# If N = 1 → Unigram\n",
    "\n",
    "# If N = 2 → Bigram\n",
    "\n",
    "# If N = 3 → Trigram\n",
    "\n",
    "# If N = 4  ->>> four gram , five gram    item can word , character, Token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb49a4",
   "metadata": {},
   "source": [
    "### Types of N-grams\n",
    "1️ Word N-grams\n",
    "\n",
    "## Based on words.\n",
    "\n",
    "Example sentence:\n",
    "\n",
    "\"I love machine learning\"\n",
    "\n",
    "Unigrams → [\"I\", \"love\", \"machine\", \"learning\"]\n",
    "\n",
    "Bigrams → [\"I love\", \"love machine\", \"machine learning\"]\n",
    "\n",
    "Trigrams → [\"I love machine\", \"love machine learning\"]\n",
    "\n",
    "### 2️ Character N-grams\n",
    "\n",
    "### Based on characters.\n",
    "\n",
    "Sentence: \"cat\"\n",
    "\n",
    "Unigrams: [\"c\", \"a\", \"t\"]\n",
    "\n",
    "Bigrams: [\"ca\", \"at\"]\n",
    "\n",
    "Trigrams: [\"cat\"]\n",
    "\n",
    "### Mostly used in:\n",
    "\n",
    "Spelling correction\n",
    "\n",
    "Language detection\n",
    "\n",
    "Text similarity\n",
    "\n",
    " Why do we use N-grams?\n",
    "\n",
    "### N-grams help NLP models understand:\n",
    "\n",
    "Word patterns\n",
    "\n",
    "Local context\n",
    "\n",
    "Sentence structure\n",
    "\n",
    "Probability of word sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e85428",
   "metadata": {},
   "source": [
    "##\n",
    "food is good\n",
    "1    0   1\n",
    "1     1    1\n",
    "\n",
    "\n",
    "\n",
    "Bigram ( combination of 2 words)\n",
    "food  not good   ->> food good \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3043cdf",
   "metadata": {},
   "source": [
    "### Applications of N-grams\n",
    "### 1. Language Modeling\n",
    "\n",
    "Predicting the next word.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"I am going to\" → next word likely \"school\"\n",
    "\n",
    " 2. Text Generation\n",
    "\n",
    "## Older chatbots and predictive keyboards used N-grams.\n",
    "\n",
    " 3. Spelling Correction \n",
    "\n",
    "\"teh\" → bigrams mismatch → suggest \"the\".\n",
    "\n",
    "### 4. Machine Translation\n",
    "\n",
    "Before neural transformers, bigram/trigram models were used.\n",
    "\n",
    "### 5. Sentiment Analysis Features\n",
    "\n",
    "Convert text into N-gram features for ML models.\n",
    "\n",
    "### 6. Text Classification (Spam, Fake news, Reviews)\n",
    "\n",
    "Common N-grams become features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca560d",
   "metadata": {},
   "source": [
    "sklearn ---> n-gram =(1,1)--> unigram (combination of one one word)\n",
    "        ---> (1,2) --> unigram , bigram \n",
    "         --> (1,3 ) --> unigram, bigram\n",
    "         -->(2,3)  ---> BIgram, trigram\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61dfde3",
   "metadata": {},
   "source": [
    "###  Problems with N-grams\n",
    "1️ Sparsity\n",
    "\n",
    "Large N means you get rare combinations.\n",
    "\n",
    "2️ High memory requirement\n",
    "\n",
    "N=4 or N=5 models require huge corpora.\n",
    "\n",
    "3️ Does not understand long context\n",
    "\n",
    "N-grams only look at immediate neighbors.\n",
    "\n",
    "Example:\n",
    "\"I went to New\" → bigram may predict \"York\",\n",
    "but cannot capture long-distance dependencies like transformers do.\n",
    "\n",
    " Smoothing in N-grams\n",
    "\n",
    "To fix zero-probability issues.\n",
    "\n",
    "Popular smoothing techniques:\n",
    "\n",
    "Laplace smoothing\n",
    "\n",
    "Add-k smoothing\n",
    "\n",
    "Kneser–Ney smoothing (best)\n",
    "\n",
    "Good-Turing smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187cb03a",
   "metadata": {},
   "source": [
    "### N-gram Features in ML Models\n",
    "\n",
    "When using classical ML algorithms (SVM, Naive Bayes, Logistic Regression), we convert text to:\n",
    "\n",
    "Bag of Unigrams\n",
    "\n",
    "Bag of Bigrams\n",
    "\n",
    "Bag of Trigrams\n",
    "\n",
    "Combination (1–2 grams)\n",
    "\n",
    "Example:\n",
    "Sentence:\n",
    "\"Good product and fast delivery\"\n",
    "\n",
    "Unigrams + bigrams become features like:\n",
    "\n",
    "good\n",
    "\n",
    "product\n",
    "\n",
    "fast\n",
    "\n",
    "delivery\n",
    "\n",
    "good product\n",
    "\n",
    "fast delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e7c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great' 'is' 'is great' 'love' 'love nlp' 'nlp' 'nlp is']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # unigrams + bigrams\n",
    "X = vectorizer.fit_transform([\"I love NLP\", \"NLP is great\"])\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2c45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character N-gram example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11fe497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f6efb",
   "metadata": {},
   "source": [
    "Good for:\n",
    "\n",
    "Plagiarism detection\n",
    "\n",
    "Language detection\n",
    "\n",
    "String matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55ef08",
   "metadata": {},
   "source": [
    "## pratical implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e153420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9856502242152466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       1.00      0.89      0.94       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.95      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Load dataset from local CSV\n",
    "df = pd.read_csv(\"SMSSpamCollection.csv\", sep='\\t', header=None, names=[\"label\", \"text\"], encoding=\"latin-1\")\n",
    "df.columns = [\"label\", \"text\"]\n",
    "\n",
    "# 2. Convert target into binary values\n",
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# 3. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Character N-grams (3 to 5 grams)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 5))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 5. Train model\n",
    "model = LogisticRegression(max_iter=300)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# 6. Predictions\n",
    "pred = model.predict(X_test_vec)\n",
    "\n",
    "# 7. Results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3decb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
